{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.metadata = pd.read_csv(csv_path)\n",
    "        self.labels = pd.factorize(self.metadata['type'])[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature_path = os.path.join('data', self.metadata.iloc[idx]['feature_path'])\n",
    "        with open(feature_path, 'rb') as f:\n",
    "            features = pickle.load(f)\n",
    "        \n",
    "        features = torch.from_numpy(features.squeeze(1)).float()\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        return features, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "def create_dataloaders(batch_size=32):\n",
    "    full_dataset = CustomDataset(csv_path='data/metadata.csv')\n",
    "    \n",
    "    total = len(full_dataset)\n",
    "    train_size = int(0.7 * total)\n",
    "    val_size = int(0.2 * total)\n",
    "    test_size = total - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        full_dataset,\n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = create_dataloaders()\n",
    "\n",
    "# 示例：获取一个batch的数据\n",
    "example_batch = next(iter(train_loader))\n",
    "print(f\"Example batch shape (features): {example_batch[0].shape}\")\n",
    "print(f\"Example batch shape (labels): {example_batch[1].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class SpectralTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_bands=128, d_model=256, \n",
    "                 nhead=4, num_layers=2, dim_feedforward=1024, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 输入特征投影层\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # 位置编码（可学习）\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(num_bands, 1, d_model))\n",
    "        \n",
    "        # Transformer编码器层\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=False  # 使用 (seq, batch, features) 格式\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 分类头\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 初始化参数\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        输入形状: (batch_size, num_bands, input_dim)\n",
    "        输出形状: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # 输入投影 [B, 128, 768] -> [B, 128, 256]\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # 调整维度为 (seq_len, batch, features) -> [128, B, 256]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        # 添加位置编码 [128, B, 256]\n",
    "        x = x + self.pos_encoder\n",
    "        \n",
    "        # Transformer编码 [128, B, 256]\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # 平均池化 [B, 256]\n",
    "        x = x.mean(dim=0)\n",
    "        \n",
    "        # 分类 [B, 2]\n",
    "        return self.classifier(x)\n",
    "\n",
    "# 初始化模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SpectralTransformer(\n",
    "    input_dim=768,\n",
    "    num_bands=128,\n",
    "    d_model=256,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    dim_feedforward=1024\n",
    ").to(device)\n",
    "\n",
    "# 测试前向传播\n",
    "example_input = torch.randn(32, 128, 768).to(device)  # 匹配示例输入形状, 并移动到设备\n",
    "output = model(example_input)\n",
    "print(f\"输出形状: {output.shape}\")  # 应输出 torch.Size([32, 2])\n",
    "\n",
    "# 创建一个示例标签\n",
    "example_labels = torch.randint(0, 2, (32,)).to(device)  # 0到1的随机整数，形状为(32,)，并移动到设备\n",
    "print(f\"Example batch shape (labels): {example_labels.shape}\")\n",
    "\n",
    "# 计算损失 (示例)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, example_labels)\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train_model, test_model, plot_training_history\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 设置训练参数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SpectralTransformer(\n",
    "    input_dim=768,\n",
    "    num_bands=128,\n",
    "    d_model=256,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    dim_feedforward=1024\n",
    ").to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "# 学习率调度器 (可选)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "# 获取数据加载器\n",
    "train_loader, val_loader, test_loader = create_dataloaders(batch_size=32)\n",
    "\n",
    "# 训练模型\n",
    "history, trained_model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=None,  # 如果使用调度器，这里改为scheduler\n",
    "    num_epochs=100,\n",
    "    device=device,\n",
    "    patience=100\n",
    ")\n",
    "\n",
    "# 保存训练好的模型 (可选)\n",
    "torch.save(trained_model.state_dict(), 'spectral_transformer_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
